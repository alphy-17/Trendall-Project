{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA67PIMeDaze"
      },
      "outputs": [],
      "source": [
        "import docx\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "\n",
        "# Initialize counters for unique identifiers\n",
        "artifact_counter = 1\n",
        "artifact_type_counter = 1\n",
        "painter_counter = 1\n",
        "location_counter = 1\n",
        "collection_counter = 1\n",
        "image_counter = 1\n",
        "chapter_counter = 1\n",
        "\n",
        "# Dictionaries to track unique entries\n",
        "artifact_type_dict = {}\n",
        "painter_dict = {}\n",
        "location_dict = {}\n",
        "collection_dict = {}\n",
        "chapter_dict = {}\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_directory = \"/content/drive/MyDrive/Trendall Project Files/Models\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_directory)\n",
        "model = BertForTokenClassification.from_pretrained(model_directory)\n",
        "\n",
        "def extract_chapter_names(full_text):\n",
        "    \"\"\"\n",
        "    Extract chapter names from the full text.\n",
        "\n",
        "    Parameters:\n",
        "    full_text (str): The complete text from the document.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of chapter names found in the text.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"CHAPTER\\s+\\d+\\..*\")\n",
        "    chapter_names = pattern.findall(full_text)\n",
        "    return chapter_names\n",
        "\n",
        "def extract_painter_names(text):\n",
        "    \"\"\"\n",
        "    Extract painter names from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text from which to extract painter names.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of painter names.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"(?<=\\n\\n)([A-Z0-9\\s\\-/'â€™:]+)(?=\\n\\n)\")\n",
        "    painter_names = [name for name in pattern.findall(text) if name.isupper()]\n",
        "    return painter_names\n",
        "\n",
        "def extract_artifact_types(text):\n",
        "    \"\"\"\n",
        "    Extract artifact types from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text from which to extract artifact types.\n",
        "\n",
        "    Returns:\n",
        "    set: A set of artifact types.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r\"(?<=\\n\\n)([A-Z][a-z\\s\\-\\(\\),]*(?:\\([a-z0-9\\s]+\\))?)(?=\\n\\n)\")\n",
        "    artifact_types = set(artifact for artifact in pattern.findall(text) if len(artifact) < 40)\n",
        "    return artifact_types\n",
        "\n",
        "def extract_dimensions(text):\n",
        "    \"\"\"\n",
        "    Extract dimensions (height and diameter) from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text from which to extract dimensions.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of dictionaries containing height and diameter.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r'(Ht\\.|ht\\.)\\s*(\\d+(?:-\\d+)?)(?:,\\s*(diam\\.|Diam\\.)\\s*(\\d+(?:-\\d+)?(?:/\\d+(?:-\\d+)?)?)?)?')\n",
        "    matches = pattern.findall(text)\n",
        "    results = []\n",
        "    for match in matches:\n",
        "        height_label, height_value, diameter_label, diameter_value = match\n",
        "        result = {}\n",
        "        result[height_label.strip('.')] = height_value\n",
        "        if diameter_label and diameter_value:\n",
        "            result[diameter_label.strip('.')] = diameter_value\n",
        "        results.append(result)\n",
        "    return results\n",
        "\n",
        "def extract_collection_location(text):\n",
        "    \"\"\"\n",
        "    Extract collection and location from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text from which to extract collection and location.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the collection and location.\n",
        "    \"\"\"\n",
        "    collection_pattern = re.compile(r'^([\\w\\s]+(?:\\([\\w\\s\\d.,]+\\))?)(?=[,\\.]\\s|\\sfrom)')\n",
        "    location_pattern = re.compile(r'from ([\\w\\s]+)')\n",
        "    collection_match = collection_pattern.search(text)\n",
        "    location_match = location_pattern.search(text)\n",
        "    collection = collection_match.group(1).strip('., ') if collection_match else None\n",
        "    location = location_match.group(1) if location_match else None\n",
        "    return collection, location\n",
        "\n",
        "def extract_artifact_entries(text):\n",
        "    \"\"\"\n",
        "    Extract artifact entries from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text from which to extract artifact entries.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of artifact entries.\n",
        "    \"\"\"\n",
        "    pattern = re.compile(r'^(\\*?\\d+[\\w/]*)$')\n",
        "    lines = text.splitlines()\n",
        "\n",
        "    collecting = False\n",
        "    collected_lines = []\n",
        "    artifact_entries = []\n",
        "\n",
        "    for i in range(len(lines)):\n",
        "        line = lines[i]\n",
        "        stripped_line = line.strip()\n",
        "\n",
        "        if not stripped_line:\n",
        "            if collecting and collected_lines:\n",
        "                artifact_entries.append(\"\\n\".join(collected_lines))\n",
        "                collected_lines = []\n",
        "                collecting = False\n",
        "            if i + 1 < len(lines):\n",
        "                next_line = lines[i + 1].strip()\n",
        "                if pattern.match(next_line):\n",
        "                    collecting = True\n",
        "                    continue\n",
        "        elif collecting:\n",
        "            collected_lines.append(line)\n",
        "\n",
        "    if collecting and collected_lines:\n",
        "        artifact_entries.append(\"\\n\".join(collected_lines))\n",
        "\n",
        "    return artifact_entries\n",
        "\n",
        "def read_artifact_files(folder_path):\n",
        "    \"\"\"\n",
        "    Read artifact files from a folder.\n",
        "\n",
        "    Parameters:\n",
        "    folder_path (str): Path to the folder containing artifact files.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary mapping file content to file paths.\n",
        "    \"\"\"\n",
        "    artifact_files = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                file_content = file.read().strip()\n",
        "                artifact_files[file_content] = file_path\n",
        "    return artifact_files\n",
        "\n",
        "def predict_entities(text):\n",
        "    \"\"\"\n",
        "    Predict entities in the text using a pre-trained BERT model.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text to predict entities.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing tokens and their corresponding labels.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    tokenized_inputs = tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_inputs).logits\n",
        "    predictions = torch.argmax(outputs, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])\n",
        "    labels = [model.config.id2label[pred.item()] for pred in predictions[0]]\n",
        "    return tokens, labels\n",
        "\n",
        "def extract_references_and_image_descriptions(text):\n",
        "    \"\"\"\n",
        "    Extract references and image descriptions from the text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): Text to extract references and image descriptions.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing references and image descriptions as strings.\n",
        "    \"\"\"\n",
        "    tokens, labels = predict_entities(text)\n",
        "    references = []\n",
        "    image_descriptions = []\n",
        "    current_ref = []\n",
        "    current_img_desc = []\n",
        "    for token, label in zip(tokens, labels):\n",
        "        if label == \"LABEL_1\":\n",
        "            if current_ref:\n",
        "                references.append(tokenizer.convert_tokens_to_string(current_ref))\n",
        "                current_ref = []\n",
        "            if token not in [\"[CLS]\", \"[SEP]\"]:\n",
        "                current_img_desc.append(token)\n",
        "        elif label == \"LABEL_2\":\n",
        "            if current_img_desc:\n",
        "                image_descriptions.append(tokenizer.convert_tokens_to_string(current_img_desc))\n",
        "                current_img_desc = []\n",
        "            if token not in [\"[CLS]\", \"[SEP]\"]:\n",
        "                current_ref.append(token)\n",
        "    if current_ref:\n",
        "        references.append(tokenizer.convert_tokens_to_string(current_ref))\n",
        "    if current_img_desc:\n",
        "        image_descriptions.append(tokenizer.convert_tokens_to_string(current_img_desc))\n",
        "\n",
        "    references_str = \" \".join(references)\n",
        "    image_descriptions_str = \" \".join(image_descriptions)\n",
        "    return references_str, image_descriptions_str\n",
        "\n",
        "def extract_artifacts_from_text(doc_path, artifact_files):\n",
        "    \"\"\"\n",
        "    Extract artifacts from a document and structure them into a database format.\n",
        "\n",
        "    Parameters:\n",
        "    doc_path (str): Path to the document containing artifact data.\n",
        "    artifact_files (dict): Dictionary mapping file content to file paths.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary representing the structured database.\n",
        "    \"\"\"\n",
        "    global artifact_counter, artifact_type_counter, painter_counter, location_counter, collection_counter, image_counter, chapter_counter\n",
        "\n",
        "    if not os.path.exists(doc_path):\n",
        "        raise FileNotFoundError(f\"File not found: {doc_path}\")\n",
        "\n",
        "    doc = docx.Document(doc_path)\n",
        "    full_text = \"\\n\".join(para.text for para in doc.paragraphs)\n",
        "    chapter_names = extract_chapter_names(full_text)\n",
        "\n",
        "    artifacts = []\n",
        "    types_of_artifacts = []\n",
        "    painters = []\n",
        "    locations = []\n",
        "    collections = []\n",
        "    images = []\n",
        "    chapters = []\n",
        "    artifact_type_relations = []\n",
        "    artifact_painter_relations = []\n",
        "    artifact_location_relations = []\n",
        "    artifact_collection_relations = []\n",
        "    artifact_chapter_relations = []\n",
        "    artifact_image_relations = []\n",
        "\n",
        "    for chapter in chapter_names:\n",
        "        chapter_start_idx = full_text.index(chapter)\n",
        "        next_chapter_idx = len(full_text)\n",
        "\n",
        "        for next_chapter in chapter_names[chapter_names.index(chapter) + 1:]:\n",
        "            try:\n",
        "                next_chapter_idx = full_text.index(next_chapter, chapter_start_idx + 1)\n",
        "                break\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "        chapter_text = full_text[chapter_start_idx:next_chapter_idx]\n",
        "        chapter_name = chapter.split('\\n')[0]\n",
        "\n",
        "        painter_names = extract_painter_names(chapter_text)\n",
        "        for painter in painter_names:\n",
        "            painter_start_idx = chapter_text.index(painter)\n",
        "            next_painter_idx = len(chapter_text)\n",
        "\n",
        "            for next_painter in painter_names[painter_names.index(painter) + 1:]:\n",
        "                try:\n",
        "                    next_painter_idx = chapter_text.index(next_painter, painter_start_idx + 1)\n",
        "                    break\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            painter_text = chapter_text[painter_start_idx:next_painter_idx]\n",
        "\n",
        "            artifact_types = extract_artifact_types(painter_text)\n",
        "            for artifact_type in artifact_types:\n",
        "                artifact_type_start_idx = painter_text.index(artifact_type)\n",
        "                next_artifact_type_idx = len(painter_text)\n",
        "\n",
        "                for next_artifact_type in artifact_types:\n",
        "                    try:\n",
        "                        next_artifact_type_idx = painter_text.index(next_artifact_type, artifact_type_start_idx + len(artifact_type))\n",
        "                        break\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                artifact_entries_text = painter_text[artifact_type_start_idx:next_artifact_type_idx]\n",
        "                artifact_entries = extract_artifact_entries(artifact_entries_text)\n",
        "\n",
        "                for entry in artifact_entries:\n",
        "                    entry = entry.strip()\n",
        "                    if entry:\n",
        "                        # Check if the artifact entry matches any file content in the dictionary\n",
        "                        if entry in artifact_files:\n",
        "                            file_path = artifact_files.pop(entry)  # Remove the matched file to prevent duplicates\n",
        "                            dimensions = extract_dimensions(entry)\n",
        "                            collection, location = extract_collection_location(entry)\n",
        "                            height = dimensions[0].get('Ht') if dimensions and 'Ht' in dimensions[0] else None\n",
        "                            diameter = dimensions[0].get('diam') if dimensions and 'diam' in dimensions[0] else None\n",
        "\n",
        "                            references, image_descriptions = extract_references_and_image_descriptions(entry)\n",
        "\n",
        "                            new_artifact = {\n",
        "                                \"artifact_id\": artifact_counter,\n",
        "                                \"fabric_name\": \"Paestum\",\n",
        "                                \"height\": height,\n",
        "                                \"diameter\": diameter,\n",
        "                                \"publications\": image_descriptions,\n",
        "                                \"image_description\": references,\n",
        "                                \"date\": None\n",
        "                            }\n",
        "\n",
        "                            artifacts.append(new_artifact)\n",
        "\n",
        "                            # Append to respective collections and create relations\n",
        "                            if artifact_type:\n",
        "                                if artifact_type not in artifact_type_dict:\n",
        "                                    artifact_type_dict[artifact_type] = artifact_type_counter\n",
        "                                    types_of_artifacts.append({\n",
        "                                        \"artifact_type_id\": artifact_type_counter,\n",
        "                                        \"artifact_type_name\": artifact_type\n",
        "                                    })\n",
        "                                    artifact_type_counter += 1\n",
        "                                artifact_type_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"artifact_type_id\": artifact_type_dict[artifact_type]\n",
        "                                })\n",
        "\n",
        "                            if painter:\n",
        "                                if painter not in painter_dict:\n",
        "                                    painter_dict[painter] = painter_counter\n",
        "                                    painters.append({\n",
        "                                        \"painter_id\": painter_counter,\n",
        "                                        \"painter_name\": painter\n",
        "                                    })\n",
        "                                    painter_counter += 1\n",
        "                                artifact_painter_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"painter_id\": painter_dict[painter]\n",
        "                                })\n",
        "\n",
        "                            if location:\n",
        "                                if location not in location_dict:\n",
        "                                    location_dict[location] = location_counter\n",
        "                                    locations.append({\n",
        "                                        \"location_id\": location_counter,\n",
        "                                        \"location_name\": location\n",
        "                                    })\n",
        "                                    location_counter += 1\n",
        "                                artifact_location_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"location_id\": location_dict[location]\n",
        "                                })\n",
        "\n",
        "                            if collection:\n",
        "                                if collection not in collection_dict:\n",
        "                                    collection_dict[collection] = collection_counter\n",
        "                                    collections.append({\n",
        "                                        \"collection_id\": collection_counter,\n",
        "                                        \"collection_name\": collection\n",
        "                                    })\n",
        "                                    collection_counter += 1\n",
        "                                artifact_collection_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"collection_id\": collection_dict[collection]\n",
        "                                })\n",
        "\n",
        "                            if chapter_name:\n",
        "                                if chapter_name not in chapter_dict:\n",
        "                                    chapter_dict[chapter_name] = chapter_counter\n",
        "                                    chapters.append({\n",
        "                                        \"chapter_id\": chapter_counter,\n",
        "                                        \"chapter_name\": chapter_name\n",
        "                                    })\n",
        "                                    chapter_counter += 1\n",
        "                                artifact_chapter_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"chapter_id\": chapter_dict[chapter_name]\n",
        "                                })\n",
        "\n",
        "                            # Handle images\n",
        "                            image_path = file_path.replace('.txt', '.jpg')  # Assuming image files have the same base name but with .jpg extension\n",
        "                            if os.path.exists(image_path):\n",
        "                                images.append({\n",
        "                                    \"image_id\": image_counter,\n",
        "                                    \"artifact_id\": artifact_counter,  # Assuming each artifact has a unique image\n",
        "                                    \"image_path\": image_path\n",
        "                                })\n",
        "                                artifact_image_relations.append({\n",
        "                                    \"artifact_id\": artifact_counter,\n",
        "                                    \"image_id\": image_counter\n",
        "                                })\n",
        "                                image_counter += 1\n",
        "\n",
        "                            artifact_counter += 1\n",
        "\n",
        "    database = {\n",
        "        \"Artifacts\": artifacts,\n",
        "        \"TypeOfArtifact\": types_of_artifacts,\n",
        "        \"Painter\": painters,\n",
        "        \"Locations\": locations,\n",
        "        \"Collection\": collections,\n",
        "        \"Images\": images,\n",
        "        \"Chapters\": chapters,\n",
        "        \"Artifact_TypeOfArtifact\": artifact_type_relations,\n",
        "        \"Artifact_Painter\": artifact_painter_relations,\n",
        "        \"Artifact_Provenances\": artifact_location_relations,\n",
        "        \"Artifact_Collection\": artifact_collection_relations,\n",
        "        \"Artifact_Chapter\": artifact_chapter_relations,\n",
        "        \"Artifact_Image\": artifact_image_relations\n",
        "    }\n",
        "\n",
        "    return database\n",
        "\n",
        "# Ensure the file paths are correct\n",
        "doc_path = \"/content/Finalised Text Extraction.docx\"\n",
        "folder_path = \"/content/drive/MyDrive/Trendall Project Files/Trendall Project/AllFiles\"  # Replace with your actual folder path\n",
        "\n",
        "# Read all artifact files into memory\n",
        "artifact_files = read_artifact_files(folder_path)\n",
        "\n",
        "# Extract artifacts and structure them in the database format\n",
        "database = extract_artifacts_from_text(doc_path, artifact_files)\n",
        "\n",
        "# Convert database to JSON format and save to file\n",
        "with open(\"DataForMongoDB.json\", \"w\") as json_file:\n",
        "    json.dump(database, json_file, indent=4)\n",
        "\n",
        "print(\"Database data saved to DataForMongoDB.json\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
